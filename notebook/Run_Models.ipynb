{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model_path = \"D:\\\\01_PYTHON\\\\05_CoderS\\\\23_Age_Gender_Prediction_VM\\\\script_models\\\\xception_age.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_age_model = load_model(age_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Import image, pre-process it before push it in model\n",
    "    :param image:\n",
    "    :return: processed image\n",
    "    \"\"\"\n",
    "    # de-noise parameter, higher is stronger\n",
    "    denoise = 5\n",
    "    processed_img = cv2.fastNlMeansDenoisingColored(image, None, denoise, 10, 7, 21)\n",
    "    ## change image to HSV color space to brighten it up\n",
    "    # hsvImg = cv2.cvtColor(processed_img, cv2.COLOR_RGB2HSV)\n",
    "    # value = 50\n",
    "    # vValue = hsvImg[..., 2]\n",
    "    # hsvImg[..., 2] = np.where((255 - vValue) < value, 255, vValue + value)\n",
    "    # processed_img = cv2.cvtColor(hsvImg, cv2.COLOR_HSV2RGB)\n",
    "    resized = cv2.resize(processed_img, (299, 299), interpolation=cv2.INTER_AREA) # resize image to 299x299, input of Xception model\n",
    "    resized = np.expand_dims(resized, axis=0)\n",
    "    img = resized / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_path = pathlib.Path(\"D:\\\\01_PYTHON\\\\05_CoderS\\\\78_Ads_Targeted_Audience\\\\datasets\\\\IMDB\\\\wiki_crop\\\\20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get file names of all jpeg files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = list(image_list_path.glob(\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read one image in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(str(image_list[63].absolute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert image to RGB because imread function of CV2 always return with BGR format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img,  cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = preprocess_image(img)\n",
    "output = xception_age_model.predict(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_class = np.argmax(output[0])\n",
    "output_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust framerate from camera\n",
    "frame_rate = 10\n",
    "prev = 0\n",
    "alpha = 1.5\n",
    "# capture video\n",
    "cap = cv2.VideoCapture(0) #use 0, 1, 2 if hardware raises error\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while True:\n",
    "    time_elapsed = time.time() - prev #\n",
    "    ret, img = cap.read() # get video frame\n",
    "#     img = cv2.rotate(img, cv2.ROTATE_180) # layout of camera on reletive positions\n",
    "\n",
    "    if time_elapsed > 1./frame_rate:\n",
    "\n",
    "        print(\"Get frame\", str(datetime.datetime.now()))\n",
    "        prev =  time.time()\n",
    "\n",
    "#         input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #change to RGB\n",
    "        height, width, channel = input_img.shape  \n",
    "        face_locations = face_recognition.face_locations(input_img)\n",
    "        \n",
    "        for face in face_locations:\n",
    "            # print(\"face detected\")\n",
    "            top, right, bottom, left = face  \n",
    "            center_y, center_x = int((top + bottom) / 2), int((right + left) / 2)\n",
    "            border = int((right - left) * alpha)\n",
    "\n",
    "            x_right, y_up = int(center_x + border / 2), int(center_y - border / 2)\n",
    "            x_left, y_down = int(center_x - border / 2), int(center_y + border / 2)\n",
    "\n",
    "            if x_left > 0 and x_left + border < width and y_up > 0 and y_up + border < height:\n",
    "                crop_face = img[y_up: y_up + border, x_left: x_left + border]\n",
    "                img_input = pre_processing_image(crop_face)\n",
    "                output = xception_age_model.predict(img_input)\n",
    "                print(output)\n",
    "                \n",
    "#             for i, d in enumerate(detected):\n",
    "#                 label = \"{}, {}\".format(int(status_age.value),\n",
    "#                                         \"M\" if status_gender.value < 0.5 else \"F\")\n",
    "#                 draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "        cv2.imshow(\"result\", img)\n",
    "        key = cv2.waitKey(30)\n",
    "\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model_path = \"D:\\\\01_PYTHON\\\\05_CoderS\\\\23_Age_Gender_Prediction_VM\\\\script_models\\\\xception_gender.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_gender_model = load_model(gender_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Import image, pre-process it before push it in model\n",
    "    :param image:\n",
    "    :return: processed image\n",
    "    \"\"\"\n",
    "    # de-noise parameter, higher is stronger\n",
    "    denoise = 5\n",
    "    processed_img = cv2.fastNlMeansDenoisingColored(image, None, denoise, 10, 7, 21)\n",
    "    ## change image to HSV color space to brighten it up\n",
    "    # hsvImg = cv2.cvtColor(processed_img, cv2.COLOR_RGB2HSV)\n",
    "    # value = 50\n",
    "    # vValue = hsvImg[..., 2]\n",
    "    # hsvImg[..., 2] = np.where((255 - vValue) < value, 255, vValue + value)\n",
    "    # processed_img = cv2.cvtColor(hsvImg, cv2.COLOR_HSV2RGB)\n",
    "    resized = cv2.resize(processed_img, (299, 299), interpolation=cv2.INTER_AREA) # resize image to 299x299, input of Xception model\n",
    "    resized = np.expand_dims(resized, axis=0)\n",
    "    img = resized / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_path = pathlib.Path(\"D:\\\\01_PYTHON\\\\05_CoderS\\\\78_Ads_Targeted_Audience\\\\datasets\\\\IMDB\\\\wiki_crop\\\\20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get file names of all jpeg files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = list(image_list_path.glob(\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read one image in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(str(image_list[63].absolute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert image to RGB because imread function of CV2 always return with BGR format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img,  cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = preprocess_image(img)\n",
    "output = xception_gender_model.predict(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust framerate from camera\n",
    "frame_rate = 10\n",
    "prev = 0\n",
    "alpha = 1.5\n",
    "# capture video\n",
    "cap = cv2.VideoCapture(0) #use 0, 1, 2 if hardware raises error\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while True:\n",
    "    time_elapsed = time.time() - prev #\n",
    "    ret, img = cap.read() # get video frame\n",
    "#     img = cv2.rotate(img, cv2.ROTATE_180) # layout of camera on reletive positions\n",
    "\n",
    "    if time_elapsed > 1./frame_rate:\n",
    "\n",
    "        print(\"Get frame\", str(datetime.datetime.now()))\n",
    "        prev =  time.time()\n",
    "\n",
    "#         input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #change to RGB\n",
    "        height, width, channel = input_img.shape  \n",
    "        face_locations = face_recognition.face_locations(input_img)\n",
    "        \n",
    "        for face in face_locations:\n",
    "            # print(\"face detected\")\n",
    "            top, right, bottom, left = face  \n",
    "            center_y, center_x = int((top + bottom) / 2), int((right + left) / 2)\n",
    "            border = int((right - left) * alpha)\n",
    "\n",
    "            x_right, y_up = int(center_x + border / 2), int(center_y - border / 2)\n",
    "            x_left, y_down = int(center_x - border / 2), int(center_y + border / 2)\n",
    "\n",
    "            if x_left > 0 and x_left + border < width and y_up > 0 and y_up + border < height:\n",
    "                crop_face = img[y_up: y_up + border, x_left: x_left + border]\n",
    "                img_input = pre_processing_image(crop_face)\n",
    "                output = xception_gender_model.predict(img_input)\n",
    "                print(output)\n",
    "                \n",
    "#             for i, d in enumerate(detected):\n",
    "#                 label = \"{}, {}\".format(int(status_age.value),\n",
    "#                                         \"M\" if status_gender.value < 0.5 else \"F\")\n",
    "#                 draw_label(img, (d.left(), d.top()), label)\n",
    "\n",
    "        cv2.imshow(\"result\", img)\n",
    "        key = cv2.waitKey(30)\n",
    "\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tensorflow model with OpenCV DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \".../..pb\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvnet = cv2.dnn.readNetFromTensorflow(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Import image, pre-process it before push it in model\n",
    "    :param image:\n",
    "    :return: processed image\n",
    "    \"\"\"\n",
    "    # de-noise parameter, higher is stronger\n",
    "    denoise = 5\n",
    "    processed_img = cv2.fastNlMeansDenoisingColored(image, None, denoise, 10, 7, 21)\n",
    "    ## change image to HSV color space to brighten it up\n",
    "    # hsvImg = cv2.cvtColor(processed_img, cv2.COLOR_RGB2HSV)\n",
    "    # value = 50\n",
    "    # vValue = hsvImg[..., 2]\n",
    "    # hsvImg[..., 2] = np.where((255 - vValue) < value, 255, vValue + value)\n",
    "    # processed_img = cv2.cvtColor(hsvImg, cv2.COLOR_HSV2RGB)\n",
    "    resized = cv2.resize(processed_img, (299, 299), interpolation=cv2.INTER_AREA) # resize image to 299x299, input of Xception model\n",
    "    resized = np.expand_dims(resized, axis=0)\n",
    "    img = resized / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"D:\\\\01_PYTHON\\\\05_CoderS\\\\78_Ads_Targeted_Audience\\\\datasets\\\\UTKface\\\\utkface\\\\gender\\\\male\\\\45_0_3_20170119201135740.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(img,1,(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvnet.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvout = cvnet.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection with OpenCV DNN instead of dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto = \"D:\\\\01_PYTHON\\\\05_CoderS\\\\23_Age_Gender_Prediction_VM\\\\app\\\\models\\\\deploy.prototxt\"\n",
    "caffe = \"D:\\\\01_PYTHON\\\\05_CoderS\\\\23_Age_Gender_Prediction_VM\\\\app\\\\models\\\\res10_300x300_ssd_iter_140000.caffemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(proto, caffe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\Users\\\\Admin\\\\Pictures\\\\Camera Roll\\\\WIN_20200209_21_01_04_Pro.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the blob through the network and obtain the detections and predictions\n",
    "print(\"[INFO] computing object detections...\")\n",
    "net.setInput(blob)\n",
    "detections = net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "alpha = 1.5\n",
    "frame = image\n",
    "for i in range(0, detections.shape[2]):\n",
    "    # extract the confidence (i.e., probability) associated with the\n",
    "    # prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    # filter out weak detections by ensuring the `confidence` is\n",
    "    # greater than the minimum confidence\n",
    "    height, width, channel = image.shape\n",
    "    if confidence > 0.5:\n",
    "        box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "        left, top, right, bottom = box.astype(\"int\")  \n",
    "        center_x = int((right + left) / 2)\n",
    "        center_y = int((top + bottom) / 2)\n",
    "        border = int((right - left) * alpha)\n",
    "        x_right, y_up = int(center_x + border / 2), int(center_y - border / 2)\n",
    "        x_left, y_down = int(center_x - border / 2), int(center_y + border / 2)\n",
    "        if x_left > 0 and x_left + border < width and y_up > 0 and y_up + border < height:\n",
    "            crop_face = image[y_up: y_up + border, x_left: x_left + border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(crop_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Import image, pre-process it before push it in model\n",
    "    :param image:\n",
    "    :return: processed image\n",
    "    \"\"\"\n",
    "    # de-noise parameter, higher is stronger\n",
    "    denoise = 5\n",
    "    processed_img = cv2.fastNlMeansDenoisingColored(image, None, denoise, 10, 7, 21)\n",
    "    ## change image to HSV color space to brighten it up\n",
    "    # hsvImg = cv2.cvtColor(processed_img, cv2.COLOR_RGB2HSV)\n",
    "    # value = 50\n",
    "    # vValue = hsvImg[..., 2]\n",
    "    # hsvImg[..., 2] = np.where((255 - vValue) < value, 255, vValue + value)\n",
    "    # processed_img = cv2.cvtColor(hsvImg, cv2.COLOR_HSV2RGB)\n",
    "    resized = cv2.resize(processed_img, (299, 299), interpolation=cv2.INTER_AREA) # resize image to 299x299, input of Xception model\n",
    "    resized = np.expand_dims(resized, axis=0)\n",
    "    img = resized / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw label function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(image, top_left, botom_right, label, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.8, thickness=2):\n",
    "    # size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    cv2.rectangle(image, top_left, botom_right, (255, 0, 0), 1)\n",
    "    cv2.putText(image, label, (top_left[0], top_left[1] - 15), font, font_scale, (255, 0, 0), 1, lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the nearest person standing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_standing(image, detections, alpha):\n",
    "    \"\"\" Comment\n",
    "    \"\"\"\n",
    "    border_nearest, center_x_nearest, center_y_nearest = 0, 0, 0  \n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        height, width, channel = image.shape\n",
    "        if confidence > 0.5:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "            \n",
    "            left, top, right, bottom = box.astype(\"int\")  \n",
    "            center_x = int((right + left) / 2)\n",
    "            center_y = int((top + bottom) / 2)\n",
    "            border = int((right - left) * alpha)\n",
    "            \n",
    "            if border > border_nearest:\n",
    "                border_nearest = border\n",
    "                center_x_nearest = center_x\n",
    "                center_y_nearest = center_y\n",
    "                \n",
    "                \n",
    "    x_right, y_up = int(center_x_nearest + border_nearest / 2), int(center_y_nearest - border_nearest / 2)\n",
    "    x_left, y_down = int(center_x_nearest - border_nearest / 2), int(center_y_nearest + border_nearest / 2)\n",
    "    \n",
    "    detected_nearest_face = False\n",
    "    nearest_face = 0\n",
    "    if x_left > 0 and x_left + border_nearest < width and y_up > 0 and y_up + border_nearest < height:\n",
    "        nearest_face = image[y_up: y_up + border_nearest, x_left: x_left + border_nearest]\n",
    "        detected_nearest_face = True\n",
    "    \n",
    "    return detected_nearest_face, nearest_face, (x_left, y_up), (x_right, y_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run camera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "#Paths to models\n",
    "gender_model_path = os.path.join(os.getcwd(), args.gender_model_path)\n",
    "age_model_path = os.path.join(os.getcwd(), args.age_model_path)\n",
    "#Load model\n",
    "xception_gender_model = load_model(gender_model_path)\n",
    "xception_age_model = load_model(age_model_path)\n",
    "#Paths to face recognition\n",
    "proto = os.path.join(os.getcwd(), \"script_models\\\\deploy.prototxt\")\n",
    "caffe = os.path.join(os.getcwd(), \"script_models\\\\res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "net = cv2.dnn.readNetFromCaffe(proto, caffe)\n",
    "\n",
    "# ID verification\n",
    "diff_threshold = -0.3 # Range[-1, 0] \n",
    "previous_vector = tf.zeros(shape=(1, 128), dtype=tf.float32)\n",
    "id_count = 0\n",
    "\n",
    "#adjust framerate from camera\n",
    "frame_rate = 25\n",
    "prev = 0\n",
    "alpha = 1.5\n",
    "# capture video\n",
    "cap = cv2.VideoCapture(0) #use 0, 1, 2 if hardware raises error\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "while True:\n",
    "    time_elapsed = time.time() - prev #\n",
    "    ret, image = cap.read() # get video frame\n",
    "\n",
    "    if time_elapsed > 1./frame_rate:\n",
    "        # print(\"Get frame\", str(datetime.datetime.now()))\n",
    "        prev =  time.time()\n",
    "        #input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #change to RGB\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        detected, crop_face, top_left, bottom_right = nearest_standing(image, detections, alpha)\n",
    "\n",
    "        content = \"\"\n",
    "        if detected:\n",
    "            img_input = preprocess_image(crop_face)\n",
    "            age_prob = xception_age_model.predict(img_input)\n",
    "            # print(\"Age: \" , np.argmax(age_prob))\n",
    "            gender_prob = xception_gender_model.predict(img_input)\n",
    "            # print(\"Gender: \", gender_prob)\n",
    "            text_gender = \"M\" if gender_prob[0][0] > 0.5 else \"F\"\n",
    "            text_age = str(np.argmax(age_prob[0]))\n",
    "\n",
    "            vector_face = age_prob[1]\n",
    "            detected_id_value = tf.keras.losses.cosine_similarity(previous_vector, vector_face).numpy()[0]\n",
    "            text_id = \"Unknown\"\n",
    "\n",
    "            if detected_id_value < diff_threshold:\n",
    "                text_id = \"Same\"\n",
    "            else:\n",
    "                text_id = \"Diff\"\n",
    "                id_count += 1\n",
    "\n",
    "            previous_vector = vector_face\n",
    "\n",
    "            content = \"G: \" + text_gender + \", R: \" + text_age + \", ID: \" + str(id_count)\n",
    "            image = draw_label(image, top_left, bottom_right, content)  \n",
    "\n",
    "    cv2.imshow(\"result\", image)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
